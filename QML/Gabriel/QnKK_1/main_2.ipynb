{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 17:27:55.201783: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 17:27:55.203752: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 17:27:55.209501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 17:27:55.218344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 17:27:55.220902: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 17:27:55.227896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 17:27:56.355243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statistics import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pickle, IPython\n",
    "\n",
    "from sklearn import datasets, preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.data import mnist_data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit.quantum_info import Statevector\n",
    "\n",
    "from skimage import io, transform, color\n",
    "from skimage.feature import graycomatrix , graycoprops\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def get_current_directory():\n",
    "    try:\n",
    "        \n",
    "        directory = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    except:\n",
    "        \n",
    "        ip = IPython.get_ipython()\n",
    "        directory = None\n",
    "        if '__vsc_ipynb_file__' in ip.user_ns:\n",
    "            directory = os.path.dirname(ip.user_ns['__vsc_ipynb_file__'])\n",
    "        \n",
    "    return directory \n",
    "\n",
    "\n",
    "def salvar(a):\n",
    "    \n",
    "    script_dir = get_current_directory()\n",
    "    \n",
    "    dados_dir = os.path.join(script_dir, 'dados')\n",
    "    \n",
    "    os.makedirs(dados_dir, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(dados_dir, f'{a}.pickle')\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(eval(a), f)\n",
    "\n",
    "def abrir(a):\n",
    "    \n",
    "    script_dir = get_current_directory()\n",
    "    \n",
    "    file_path = os.path.join(script_dir, 'dados', f'{a}.pickle')\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def salvar_obj(obj, filename):\n",
    "    script_dir = get_current_directory()\n",
    "    dados_dir = os.path.join(script_dir, 'dados')\n",
    "    os.makedirs(dados_dir, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(dados_dir, f'{filename}.pickle')\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making features vectors (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_h(hue):\n",
    "    return int(hue // 40)  \n",
    "\n",
    "def quantize_s(saturation):\n",
    "    return int(saturation // 0.33)  \n",
    "\n",
    "def quantize_b(brightness):\n",
    "    return int(brightness // 0.33)  \n",
    "\n",
    "angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "\n",
    "def extract_features(X):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for idx in range(X.shape[0]):\n",
    "\n",
    "        img = X[idx].reshape(28, 28)\n",
    "        \n",
    "\n",
    "        gray_img = img\n",
    "        \n",
    "\n",
    "        hsv_img = color.rgb2hsv(np.stack((gray_img,)*3, axis=-1))  \n",
    "        hue = hsv_img[:, :, 0] * 360\n",
    "        saturation = hsv_img[:, :, 1]\n",
    "        brightness = hsv_img[:, :, 2]\n",
    "        \n",
    "        h_quantized = np.vectorize(quantize_h)(hue)\n",
    "        s_quantized = np.vectorize(quantize_s)(saturation)\n",
    "        b_quantized = np.vectorize(quantize_b)(brightness)\n",
    "        \n",
    "        Q_h = 9 \n",
    "        Q_s = 3\n",
    "        G = h_quantized * Q_h + s_quantized * Q_s + b_quantized\n",
    "        \n",
    "        \n",
    "        gray_img_uint = (gray_img * 255).astype(np.uint8)\n",
    "        glcm = graycomatrix(gray_img_uint, distances=[1], angles=angles)\n",
    "            \n",
    "        contrast = graycoprops(glcm, 'contrast')\n",
    "        correlation = graycoprops(glcm, 'correlation')\n",
    "        energy = graycoprops(glcm, 'energy')\n",
    "        \n",
    "        \n",
    "        entropy = -np.sum(glcm * np.log2(glcm + (glcm == 0)), axis=(0, 1))\n",
    "        \n",
    "        glcm_features = []\n",
    "        for prop in [contrast, correlation, energy, entropy]:\n",
    "            mean = np.mean(prop)\n",
    "            variance = np.var(prop)\n",
    "            glcm_features.extend([mean, variance])\n",
    "        \n",
    "        G_histogram, _ = np.histogram(G, bins=np.arange(73))\n",
    "        \n",
    "        feature_vector = np.concatenate([G_histogram, glcm_features])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "X = extract_features(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abrir('features_train')\n",
    "y = abrir('train_y')\n",
    "train_X = abrir('train_X')\n",
    "test_X = abrir('test_X')\n",
    "test_y = abrir('test_y')\n",
    "features_test = abrir('features_test')\n",
    "\n",
    "M = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: (60000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Extracted features shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = extract_features(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization with Pair Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "min_max_scaler.fit(X)\n",
    "X_normalized = min_max_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Sample Selection and Quantum State Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 57.4 GiB for an array with shape (2, 32, 28, 17210368) and data type float16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     phi \u001b[38;5;241m=\u001b[39m phi_i\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mkron(phi_i, phi)\n",
      "File \u001b[0;32m~/miniconda/envs/aws_braket_updated/lib/python3.12/site-packages/numpy/lib/shape_base.py:1173\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1171\u001b[0m b_arr \u001b[38;5;241m=\u001b[39m expand_dims(b_arr, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, nd\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;66;03m# In case of `mat`, convert result to `array`\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m result \u001b[38;5;241m=\u001b[39m _nx\u001b[38;5;241m.\u001b[39mmultiply(a_arr, b_arr, subok\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m is_any_mat))\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# Reshape back\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(_nx\u001b[38;5;241m.\u001b[39mmultiply(as_, bs))\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 57.4 GiB for an array with shape (2, 32, 28, 17210368) and data type float16"
     ]
    }
   ],
   "source": [
    "test_index = int(np.random.rand()*len(test_X)) \n",
    "phi_test = test_X[test_index]\n",
    "\n",
    "# Create the encoded feature vector\n",
    "for i in range(M):\n",
    "    phi_i = [np.sqrt(phi_test[i]), np.sqrt(1- phi_test[i])]\n",
    "    if i == 0:\n",
    "        phi = phi_i\n",
    "    else:\n",
    "        phi = np.kron(phi_i, phi)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_braket_updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
